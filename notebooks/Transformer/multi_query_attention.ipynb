{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d030c27-7957-4e2c-9833-625ded049b40",
   "metadata": {},
   "source": [
    "# Multi Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06f0d44c-8f6c-4339-bd89-dc999f745f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7e4cae77-0b9f-4bbb-960e-961a41624a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 6\n",
    "n_embed = 48\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "bias = False\n",
    "dropout = 0.2\n",
    "\n",
    "x = torch.tensor(np.random.rand(batch_size, block_size, n_embed), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfdb67-c401-4e52-aeac-08751fe428a0",
   "metadata": {},
   "source": [
    "## LL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17c209ba-93e9-4388-8d46-a877059a66d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention_LL1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int,\n",
    "        n_embed: int,\n",
    "        n_head: int,\n",
    "        n_query_groups: int,\n",
    "        dropout: float,\n",
    "        bias: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.block_size = block_size\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.n_query_groups = n_query_groups\n",
    "        self.hs = n_embed // n_head\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, (n_head + 2 * n_query_groups) * self.hs, bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_residual = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        q_per_kv = self.n_head // self.n_query_groups\n",
    "        total_qkv = q_per_kv + 2\n",
    "        qkv = qkv.view(B, T, self.n_query_groups, total_qkv, self.hs)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # B, n_query_groups, total_qkv, T, hs\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "\n",
    "        q = q.contiguous().view(B, -1, T, self.hs)\n",
    "        k = k.contiguous().view(B, -1, T, self.hs)\n",
    "        v = v.contiguous().view(B, -1, T, self.hs)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout_residual(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "MultiHeadAttention_LL1(\n",
    "    block_size=block_size,\n",
    "    n_embed=n_embed,\n",
    "    n_head=n_head,\n",
    "    n_query_groups=1,  # Multi-Query-Attention\n",
    "    dropout=0.2,\n",
    "    bias=False,\n",
    ")(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd86e1-3b10-472a-8f4a-c944ae958a72",
   "metadata": {},
   "source": [
    "## LL2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d054a7b-5b7e-4356-afdc-e100b80de36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalAttention_LL2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int,\n",
    "        n_embed: int,\n",
    "        n_head: int,\n",
    "        dropout: float,\n",
    "        bias: bool,\n",
    "        n_query_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.n_embed = n_embed\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.n_head = n_head\n",
    "        self.hs = n_embed // n_head\n",
    "        self.n_query_groups = n_query_groups\n",
    "\n",
    "        shape = (n_head + 2 * n_query_groups) * self.hs\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, shape, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_residual = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q_per_kv = self.n_head // self.n_query_groups\n",
    "        total_qkv = q_per_kv + 2\n",
    "        qkv = qkv.view(B, T, self.n_query_groups, total_qkv, self.hs)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # B, n_query_groups, total_qkv, T, hs\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "\n",
    "        q = q.contiguous().view(B, -1, T, self.hs)\n",
    "        k = k.contiguous().view(B, -1, T, self.hs)\n",
    "        v = v.contiguous().view(B, -1, T, self.hs)\n",
    "\n",
    "        # attn = (q @ k.transpose(-2, -1)) * (1 / math.sqrt(k.size(-1)))\n",
    "        # attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        # attn = F.softmax(attn, dim=-1)\n",
    "        # attn = self.dropout_attn(attn)\n",
    "\n",
    "        attn = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            scale=1 / math.sqrt(k.size(-1)),\n",
    "            is_causal=True,\n",
    "            dropout_p=self.dropout if self.training else 0,\n",
    "        )\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        y = self.dropout_residual(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "CausalAttention_LL2(\n",
    "    block_size=block_size, n_embed=n_embed, n_head=n_head, dropout=0.2, bias=False\n",
    ")(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e424c5-9766-45fb-8642-b85c10be3f5d",
   "metadata": {},
   "source": [
    "## LL2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d747f95-a80d-48fd-a9b2-e1ac08aa0d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalAttention_LL22(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int,\n",
    "        n_embed: int,\n",
    "        n_head: int,\n",
    "        dropout: float,\n",
    "        bias: bool,\n",
    "        n_query_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.dropout = dropout\n",
    "        self.hs = n_embed // n_head\n",
    "        self.n_query_groups = n_query_groups\n",
    "\n",
    "        shape = (self.n_head + 2 * self.n_query_groups) * self.hs\n",
    "        self.c_attn = nn.Linear(n_embed, shape, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_residual = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q_per_kv = self.n_head // self.n_query_groups\n",
    "        total_qkv = q_per_kv + 2\n",
    "        qkv = qkv.view(B, T, self.n_query_groups, total_qkv, self.hs)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)  # B, n_query_groups, total_qkv, T, hs\n",
    "\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "        q = q.contiguous().view(B, -1, T, self.hs)\n",
    "        k = k.contiguous().view(B, -1, T, self.hs)\n",
    "        v = v.contiguous().view(B, -1, T, self.hs)\n",
    "\n",
    "        # attn = F.scaled_dot_product_attention(\n",
    "        #    q,\n",
    "        #    k,\n",
    "        #    v,\n",
    "        #    is_causal=True,\n",
    "        #    dropout_p=self.dropout if self.training else 0,\n",
    "        #    scale=1.0 / math.sqrt(k.size(-1)),\n",
    "        # )\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1 / math.sqrt(k.size(-1)))\n",
    "        attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout_residual(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "CausalAttention_LL22(\n",
    "    block_size=block_size, n_embed=n_embed, n_head=n_head, dropout=0.2, bias=False\n",
    ")(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17eff1b-7d6e-4638-b649-1e154e8d5714",
   "metadata": {},
   "source": [
    "## LL29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9b42979f-374a-4ad6-8e6c-1b4d490a2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.size()=torch.Size([4, 6, 8, 8]) k.size()=torch.Size([4, 1, 8, 8]) v.size()=torch.Size([4, 1, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CausalAttention_LL29(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int,\n",
    "        n_embed: int,\n",
    "        n_head: int,\n",
    "        dropout: float,\n",
    "        bias: bool,\n",
    "        n_query_groups: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.hs = n_embed // n_head\n",
    "        self.n_embed = n_embed\n",
    "        self.dropout = dropout\n",
    "        self.n_query_groups = n_query_groups\n",
    "        self.n_head = n_head\n",
    "\n",
    "        shape = (self.n_head + 2 * self.n_query_groups) * self.hs\n",
    "        self.c_attn = nn.Linear(n_embed, shape, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_residual = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q_per_kv = self.n_head // self.n_query_groups\n",
    "        total_qkv = q_per_kv + 2\n",
    "        qkv = qkv.view(B, T, self.n_query_groups, total_qkv, self.hs)\n",
    "        qkv = qkv.permute(0, 2, 3, 1, 4)\n",
    "        q, k, v = qkv.split((q_per_kv, 1, 1), dim=2)\n",
    "\n",
    "        q = q.contiguous().view(\n",
    "            B, -1, T, self.hs\n",
    "        )  # B, nh_q(q_per_kv * n_query_groups), T, hs\n",
    "        k = k.contiguous().view(\n",
    "            B, -1, T, self.hs\n",
    "        )  # B, nh_k(q_per_kv * n_query_groups), T, hs\n",
    "        v = v.contiguous().view(\n",
    "            B, -1, T, self.hs\n",
    "        )  # B, nh_v(q_per_kv * n_query_groups), T, hs\n",
    "        print(f\"{q.size()=} {k.size()=} {v.size()=}\")\n",
    "\n",
    "        # attn = (q @ k.transpose(-2, -1)) * (1 / math.sqrt(self.k.size(-1)))\n",
    "        # attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        # attn = F.softmax(attn, dim=-1)\n",
    "        # attn = self.dropout_attn(attn)\n",
    "        attn = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            is_causal=True,\n",
    "            scale=1 / math.sqrt(k.size(-1)),\n",
    "            dropout_p=self.dropout if self.training else 0,\n",
    "        )\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout_residual(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "CausalAttention_LL29(\n",
    "    block_size=block_size,\n",
    "    n_embed=n_embed,\n",
    "    n_head=n_head,\n",
    "    dropout=0.2,\n",
    "    bias=False,\n",
    "    n_query_groups=1,\n",
    ")(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
