{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de836394-d9ef-4cc0-ab02-3a96b552c8b7",
   "metadata": {},
   "source": [
    "# Rotary Positional Embedding(RoPE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e962d8c-384d-4417-8efa-560a0a113e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "380cf0e6-9f7e-4731-a822-cbb1ded6e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "dtype = torch.float16\n",
    "\n",
    "n_head = 6\n",
    "n_embed = 48\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "rope_n_elem = int(0.25 * (n_embed // n_head))\n",
    "base = 10000\n",
    "\n",
    "x = torch.tensor(\n",
    "    np.random.rand(batch_size, n_head, block_size, n_embed // n_head),\n",
    "    dtype=dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2831de-53d7-4da7-a160-008b39015070",
   "metadata": {},
   "source": [
    "## LL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "16a8e1f8-0c07-420b-97c8-558445b447cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 8, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rope_cache_ll1(\n",
    "    block_size: int,\n",
    "    n_elem: int,\n",
    "    base: int = 10000,\n",
    "    condense_ratio: int = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "):\n",
    "    theta = 1.0 / (\n",
    "        base ** (torch.arange(0, n_elem, 2, device=device, dtype=dtype) / n_elem)\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_ll1(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., : hs // 2]  # B, nh, T, hs/2\n",
    "    x2 = x[..., hs // 2 :]  # B, nh, T, hs/2\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)  # B, nh, T, hs\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "cos, sin = build_rope_cache_ll1(\n",
    "    block_size=block_size, n_elem=rope_n_elem, base=base, device=device, dtype=dtype\n",
    ")\n",
    "\n",
    "apply_rope_ll1(x[..., :rope_n_elem], cos, sin).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1066c4f-99df-4358-8aa3-3e586e7c6554",
   "metadata": {},
   "source": [
    "## LL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "10646b88-0233-479f-9657-389a1791f5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 8, 2])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rope_cache_ll2(\n",
    "    block_size: int,\n",
    "    rope_n_elem: int,\n",
    "    base: int = 10000,\n",
    "    condense_ratio: int = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "):\n",
    "    theta = 1 / base ** (\n",
    "        torch.arange(0, rope_n_elem, 2, device=device, dtype=dtype) / rope_n_elem\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device, dtype=dtype) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_ll2(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., : hs // 2]\n",
    "    x2 = x[..., hs // 2 :]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "cos, sin = build_rope_cache_ll2(\n",
    "    block_size=block_size,\n",
    "    rope_n_elem=rope_n_elem,\n",
    "    base=base,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "apply_rope_ll2(x[..., :rope_n_elem], cos, sin).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ace9c-95db-4d79-a20f-cdd92142235d",
   "metadata": {},
   "source": [
    "## LL25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8f9dc075-1c34-4078-ac72-662fbec89bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 8, 2])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rope_cache_ll25(\n",
    "    block_size: int,\n",
    "    rope_n_elem: int,\n",
    "    base: int = 10000,\n",
    "    condense_ratio: int = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "):\n",
    "    theta = 1 / base ** (\n",
    "        torch.arange(0, rope_n_elem, 2, device=device, dtype=dtype) / rope_n_elem\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device, dtype=dtype) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)  # block_size, rope_n_elem\n",
    "\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_ll25(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., :hs]\n",
    "    x2 = x[..., hs:]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "cos, sin = build_rope_cache_ll25(\n",
    "    block_size=block_size,\n",
    "    rope_n_elem=rope_n_elem,\n",
    "    base=base,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "apply_rope_ll25(x[..., :rope_n_elem], cos, sin).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd6d629-d934-4414-bce2-88c54922c297",
   "metadata": {},
   "source": [
    "## LL29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b5a88365-e015-433b-b76e-50daa0975d4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 8, 2])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rope_cache_ll29(\n",
    "    block_size: int,\n",
    "    rope_n_elem: int,\n",
    "    condense_ratio: int = 1,\n",
    "    base: int = 10000,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    "):\n",
    "    theta = 1 / base ** (\n",
    "        torch.arange(0, rope_n_elem, 2, device=device, dtype=dtype) / rope_n_elem\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device, dtype=dtype) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_ll29(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., : hs // 2]\n",
    "    x2 = x[..., hs // 2 :]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "cos, sin = build_rope_cache_ll29(\n",
    "    block_size=block_size,\n",
    "    rope_n_elem=rope_n_elem,\n",
    "    base=base,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "apply_rope_ll29(x[..., :rope_n_elem], cos, sin).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c9918-5b23-4525-8242-75dc46d5cc15",
   "metadata": {},
   "source": [
    "## LL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c152aeaf-1816-462f-937f-f8970f414c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 8, 2])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_rope_cache_ll3(\n",
    "    block_size: int,\n",
    "    rope_n_elem: int,\n",
    "    base: Optional[int] = 10000,\n",
    "    condense_ratio: Optional[int] = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    theta = 1 / base ** (\n",
    "        torch.arange(0, rope_n_elem, 2, device=device, dtype=dtype) / rope_n_elem\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device, dtype=dtype) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_ll3(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., : hs // 2]\n",
    "    x2 = x[..., hs // 2 :]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "cos, sin = build_rope_cache_ll3(\n",
    "    block_size=block_size,\n",
    "    rope_n_elem=rope_n_elem,\n",
    "    base=base,\n",
    "    device=device,\n",
    "    dtype=dtype,\n",
    ")\n",
    "\n",
    "apply_rope_ll3(x[..., :rope_n_elem], cos, sin).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f7b1c-36d0-4325-99f0-80668a518e23",
   "metadata": {},
   "source": [
    "# MultiHeadAttention With RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c5b6830e-be6b-4da9-84ec-bcd929643181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "dtype = torch.float16\n",
    "\n",
    "n_head = 6\n",
    "n_embed = 48\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "rope_n_elem = int(0.25 * (n_embed // n_head))\n",
    "base = 10000\n",
    "\n",
    "x = torch.tensor(\n",
    "    np.random.rand(batch_size, block_size, n_embed), dtype=dtype, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460adcad-d99f-4131-aa28-7ddb85b9b173",
   "metadata": {},
   "source": [
    "## LL1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e51b6f90-479a-4dde-a59d-7c5b82fe61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache_mhall1(\n",
    "    block_size: int,\n",
    "    rope_n_elem: int,\n",
    "    base: Optional[int] = 10000,\n",
    "    condense_ratio: Optional[int] = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[torch.dtype] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    theta = 1 / base ** (\n",
    "        torch.arange(0, rope_n_elem, 2, device=device, dtype=dtype) / rope_n_elem\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device, dtype=dtype) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_mhall1(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., : hs // 2]\n",
    "    x2 = x[..., hs // 2 :]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLL1(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int,\n",
    "        n_head: int,\n",
    "        n_embed: int,\n",
    "        dropout: float = 0.20,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_residual = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(n_embed, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        q_roped = apply_rope_mhall1(q[..., :rope_n_elem], cos, sin)\n",
    "        k_roped = apply_rope_mhall1(k[..., :rope_n_elem], cos, sin)\n",
    "        q = torch.cat((q_roped, q[..., rope_n_elem:]), dim=-1)\n",
    "        k = torch.cat((k_roped, k[..., rope_n_elem:]), dim=-1)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout_residual(self.c_proj(y))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c2760e3c-58df-4211-97a5-c8ee9ae01944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin = build_rope_cache_mhall1(\n",
    "    block_size=block_size, rope_n_elem=rope_n_elem, device=device, dtype=dtype\n",
    ")\n",
    "\n",
    "y = MultiHeadAttentionLL1(block_size=block_size, n_embed=n_embed, n_head=n_head)(\n",
    "    x, cos, sin\n",
    ")\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da754e5e-caa5-4052-b785-af35f325736a",
   "metadata": {},
   "source": [
    "## LL3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1ec44dd1-16d4-419b-85c2-81e9add8d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rope_cache_mhall3(\n",
    "    block_size: int,\n",
    "    rope_n_elem: int,\n",
    "    base: Optional[int] = 10000,\n",
    "    condense_ratio: Optional[int] = 1,\n",
    "    device: Optional[torch.device] = None,\n",
    "    dtype: Optional[dtype] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    theta = 1 / base ** (\n",
    "        torch.arange(0, rope_n_elem, 2, device=device, dtype=dtype) / rope_n_elem\n",
    "    )\n",
    "    seq_idx = torch.arange(block_size, device=device, dtype=dtype) / condense_ratio\n",
    "    idx_theta = torch.outer(seq_idx, theta).repeat(1, 2)\n",
    "    return torch.cos(idx_theta), torch.sin(idx_theta)\n",
    "\n",
    "\n",
    "def apply_rope_mhall3(\n",
    "    x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    hs = x.size(-1)\n",
    "    x1 = x[..., : hs // 2]\n",
    "    x2 = x[..., hs // 2 :]\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "    roped = (x * cos) + (rotated * sin)\n",
    "    return roped.type_as(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLL3(nn.Module):\n",
    "    def __init__(\n",
    "        self, block_size: int, n_embed: int, n_head: int, dropout: float, bias: bool\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.n_embed = n_embed\n",
    "        self.n_head = n_head\n",
    "        self.block_size = block_size\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_residual = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        q_roped = apply_rope_mhall3(q[..., :rope_n_elem], cos, sin)\n",
    "        k_roped = apply_rope_mhall3(k[..., :rope_n_elem], cos, sin)\n",
    "        q = torch.cat((q_roped, q[..., rope_n_elem:]), dim=-1)\n",
    "        k = torch.cat((k_roped, k[..., rope_n_elem:]), dim=-1)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) / math.sqrt(k.size(-1))\n",
    "        attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout_residual(self.c_proj(y))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1b154e01-78ce-4855-89ee-f718b41dbf77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos, sin = build_rope_cache_mhall3(block_size=block_size, rope_n_elem=rope_n_elem)\n",
    "\n",
    "y = MultiHeadAttentionLL3(\n",
    "    block_size=block_size, n_embed=n_embed, n_head=n_head, dropout=0.2, bias=False\n",
    ")(x, cos, sin)\n",
    "\n",
    "y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
