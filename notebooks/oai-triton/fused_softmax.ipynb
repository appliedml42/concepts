{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf42566-890c-4ee0-a624-2b04b58cbf2e",
   "metadata": {},
   "source": [
    "# Fused Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664e44c8-98d3-4d17-b73d-4b7ef22a4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acb0704-d7ac-41ae-83db-414cd024c1ce",
   "metadata": {},
   "source": [
    "## L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf3a4d6-6c40-4b31-9fd2-0ef2839caaae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def naive_softmax(x):\n",
    "    # read: MN, write: M\n",
    "    x_max = x.max(dim=1)[0].unsqueeze(1)\n",
    "    # read: MN + M, write: MN\n",
    "    z = x - x_max\n",
    "    # read: MN, write: MN\n",
    "    numerator = torch.exp(z)\n",
    "    # read: MN, write M\n",
    "    denominator = numerator.sum(dim=1).unsqueeze(1)\n",
    "    # read: MN + M, write MN\n",
    "    ret = numerator / denominator\n",
    "    # total read:5MN + 2M, write: 3MN + 2M\n",
    "    return ret\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr,\n",
    "    output_ptr,\n",
    "    input_row_stride,\n",
    "    output_row_stride,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    row_idx = tl.program_id(axis=0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    input_ptrs = input_ptr + row_idx * input_row_stride + col_offsets\n",
    "    x = tl.load(input_ptrs, mask=mask, other=float(\"-inf\"))\n",
    "\n",
    "    x_minus_max = x - tl.max(x, axis=0)\n",
    "    num = tl.exp(x_minus_max)\n",
    "    deno = tl.sum(num, axis=0)\n",
    "    y = num / deno\n",
    "\n",
    "    output_ptrs = output_ptr + row_idx * output_row_stride + col_offsets\n",
    "    tl.store(output_ptrs, y, mask=mask)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    n_rows, n_cols = x.size()\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "\n",
    "    y = torch.empty_like(x)\n",
    "    softmax_kernel[(n_rows,)](\n",
    "        x,\n",
    "        y,\n",
    "        x.stride(0),\n",
    "        y.stride(0),\n",
    "        n_cols,\n",
    "        num_warps=num_warps,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1823, 781, device=\"cuda\")\n",
    "y_naive = naive_softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "y_triton = softmax(x)\n",
    "assert torch.allclose(y_torch, y_triton), (y_naive, y_triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d12f2-51fa-47c7-8beb-4e5943129c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"N\"],\n",
    "        x_vals=[128 * x for x in range(2, 100)],\n",
    "        line_arg=\"provider\",\n",
    "        line_vals=[\"triton\", \"torch-native\", \"torch-naive\"],\n",
    "        line_names=[\"Triton\", \"Torch (native)\", \"Torch (jit)\"],\n",
    "        styles=[(\"blue\", \"-\"), (\"green\", \"-\"), (\"red\", \"-\")],\n",
    "        ylabel=\"GB/sec\",\n",
    "        plot_name=\"softmax-performance-plot\",\n",
    "        args={\"M\": 4096},\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == \"torch-native\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, axis=1), quantiles=quantiles\n",
    "        )\n",
    "    if provider == \"torch-naive\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: naive_softmax(x), quantiles=quantiles\n",
    "        )\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax(x), quantiles=quantiles\n",
    "        )\n",
    "\n",
    "    gbps = lambda ms: 2 * (x.numel() * x.element_size() / ms) * 1e-6\n",
    "    return gbps(ms), gbps(min_ms), gbps(max_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033837a-cdbb-42a7-9920-6726e6b80383",
   "metadata": {},
   "source": [
    "## L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd28d1d-8585-4ad5-8569-d7300e5f8fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def softmax_naive(x):\n",
    "    # read: MN, write: M\n",
    "    x_max = torch.max(x, dim=1)[0].unsqueeze(1)\n",
    "\n",
    "    # read: MN + M, write: MN\n",
    "    x_min_max = x - x_max\n",
    "\n",
    "    # read: MN, write: MN\n",
    "    num = torch.exp(x_min_max)\n",
    "\n",
    "    # read: MN, write: M\n",
    "    deno = torch.sum(num, dim=1).unsqueeze(1)\n",
    "\n",
    "    # read: MN + M, write: MN\n",
    "    y = num / deno\n",
    "\n",
    "    # read: 5MN + 2M, write: 3MN + 2M\n",
    "    return y\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr,\n",
    "    output_ptr,\n",
    "    input_row_stride,\n",
    "    output_row_stride,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    row_id = tl.program_id(axis=0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    input_ptrs = input_ptr + row_id * input_row_stride + col_offsets\n",
    "    x = tl.load(input_ptrs, mask=mask, other=float(\"-inf\"))\n",
    "\n",
    "    x_minus_max = x - tl.max(x, axis=0)\n",
    "    num = tl.exp(x_minus_max)\n",
    "    deno = tl.sum(num, axis=0)\n",
    "    y = num / deno\n",
    "\n",
    "    output_ptrs = output_ptr + row_id * output_row_stride + col_offsets\n",
    "    tl.store(output_ptrs, y, mask=mask)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    rows, cols = x.size()\n",
    "    BLOCK_SIZE = triton.next_power_of_2(cols)\n",
    "\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "\n",
    "    y = torch.empty_like(x)\n",
    "    softmax_kernel[(rows,)](\n",
    "        x, y, x.stride(0), y.stride(0), cols, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps\n",
    "    )\n",
    "    return y\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1823, 781, device=\"cuda\")\n",
    "y_naive = naive_softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "assert torch.allclose(y_naive, y_torch)\n",
    "y_triton = softmax(x)\n",
    "assert torch.allclose(y_triton, y_torch), (y_triton, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a027c73-3f03-4b00-a7bd-3509741f5e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"N\"],\n",
    "        x_vals=[128 * x for x in range(2, 100)],\n",
    "        line_arg=\"provider\",\n",
    "        line_vals=[\"triton\", \"torch-naive\", \"torch-native\"],\n",
    "        line_names=[\"Triton\", \"Torch (jit)\", \"Torch (native)\"],\n",
    "        styles=[(\"red\", \"-\"), (\"blue\", \"-\"), (\"green\", \"-\")],\n",
    "        ylabel=\"GB/sec\",\n",
    "        plot_name=\"softmax-performance-plot\",\n",
    "        args={\"M\": 4096},\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax(x), quantiles=quantiles\n",
    "        )\n",
    "    if provider == \"torch-native\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, axis=1), quantiles=quantiles\n",
    "        )\n",
    "    if provider == \"torch-naive\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax_naive(x), quantiles=quantiles\n",
    "        )\n",
    "\n",
    "    gbps = lambda ms: 2 * (x.numel() * x.element_size() / ms) * 1e-6\n",
    "    return gbps(ms), gbps(min_ms), gbps(max_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08a51c-b527-4308-ab31-97ab6a577113",
   "metadata": {},
   "source": [
    "## L2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e65d80-e7fa-40ee-998a-f9929b8ea944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1823, 781])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def naive_softmax(x):\n",
    "    # read: MN, write: M\n",
    "    x_max = x.max(dim=1)[0].unsqueeze(1)\n",
    "\n",
    "    # read: MN, write: MN\n",
    "    x = x - x_max\n",
    "\n",
    "    # read: MN, write: MN\n",
    "    num = torch.exp(x)\n",
    "\n",
    "    # read: MN, write: M\n",
    "    den = torch.sum(num, dim=1).unsqueeze(1)\n",
    "\n",
    "    # read: MN + M, write: MN\n",
    "    y = num / den\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr,\n",
    "    output_ptr,\n",
    "    input_row_stride,\n",
    "    output_row_stride,\n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    idx = tl.program_id(axis=0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    input_ptrs = input_ptr + idx * input_row_stride + col_offsets\n",
    "    x = tl.load(input_ptrs, mask=mask, other=float(\"-inf\"))\n",
    "\n",
    "    x_max = tl.max(x, axis=0)\n",
    "    x_minus_max = x - x_max\n",
    "    num = tl.exp(x_minus_max)\n",
    "    den = tl.sum(num, axis=0)\n",
    "    y = num / den\n",
    "\n",
    "    output_ptrs = output_ptr + idx * output_row_stride + col_offsets\n",
    "    tl.store(output_ptrs, y, mask=mask)\n",
    "\n",
    "\n",
    "def softmax(x: torch.Tensor):\n",
    "    row, col = x.size()\n",
    "    BLOCK_SIZE = triton.next_power_of_2(col)\n",
    "\n",
    "    num_warps = 4\n",
    "    if BLOCK_SIZE >= 2048:\n",
    "        num_warps = 8\n",
    "    if BLOCK_SIZE >= 4096:\n",
    "        num_warps = 16\n",
    "\n",
    "    y = torch.empty_like(x)\n",
    "    softmax_kernel[(row,)](\n",
    "        x, y, x.stride(0), y.stride(0), col, num_warps=num_warps, BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(1823, 781, device=\"cuda\")\n",
    "y_naive = naive_softmax(x)\n",
    "y_torch = torch.softmax(x, axis=1)\n",
    "y_triton = softmax(x)\n",
    "print(y_triton.size())\n",
    "assert torch.allclose(y_naive, y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3d273-9775-43bc-9da7-86f0cf5962e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"N\"],\n",
    "        x_vals=[128 * x for x in range(2, 100)],\n",
    "        line_arg=\"provider\",\n",
    "        line_vals=[\"triton\", \"torch-jit\", \"torch-native\"],\n",
    "        line_names=[\"Triton\", \"Torch (jit)\", \"Torch (native)\"],\n",
    "        styles=[(\"red\", \"-\"), (\"blue\", \"-\"), (\"green\", \"-\")],\n",
    "        ylabel=\"GB/sec\",\n",
    "        plot_name=\"softmax-performance-plot\",\n",
    "        args={\"M\": 4096},\n",
    "    )\n",
    ")\n",
    "def benchmark(M, N, provider):\n",
    "    x = torch.randn(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: softmax(x), quantiles=quantiles\n",
    "        )\n",
    "    if provider == \"torch-jit\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: naive_softmax(x), quantiles=quantiles\n",
    "        )\n",
    "    if provider == \"torch-native\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: torch.softmax(x, axis=1), quantiles=quantiles\n",
    "        )\n",
    "\n",
    "    gbps = lambda ms: 2 * (x.numel() * x.element_size() / ms) * 1e-6\n",
    "    return gbps(ms), gbps(min_ms), gbps(max_ms)\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
