{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d71302-97c0-4564-ba75-1a5b5cb3ba15",
   "metadata": {},
   "source": [
    "# Vanilla MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140c1c55-5795-4c39-9910-d4521bff8ee0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977b9875-d828-40c5-b983-83e80efa306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 6\n",
    "n_embed = 48\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "bias = False\n",
    "dropout = 0.2\n",
    "\n",
    "x = torch.tensor(np.random.rand(batch_size, block_size, n_embed), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59fdb5-4318-4d64-8296-331110c3ec9a",
   "metadata": {},
   "source": [
    "## Learn Level 3: No Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8654383-c693-4b17-b426-b9b2992b5388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionL3(nn.Module):\n",
    "    def __init__(self, block_size, n_head, n_embed, bias, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "        self.bias = bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(self.block_size, self.block_size)).view(\n",
    "            1, 1, self.block_size, self.block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        q, k, v = self.c_attn(x).split(\n",
    "            self.n_embed, dim=2\n",
    "        )  # B, T, C @ C, 3 x C ; O(BTC^2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * (\n",
    "            1 / math.sqrt(k.size(-1))\n",
    "        )  # B, nh, T, hs @ B, nh, hs, T; O(BnhhsT^2)\n",
    "        attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.residual_dropout(self.c_proj(y))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c965620-5ed8-4343-a7a4-c174a8ddfefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yl3 = MultiHeadAttentionL3(\n",
    "    block_size=block_size, n_embed=n_embed, n_head=n_head, bias=bias, dropout=dropout\n",
    ")(x)\n",
    "yl3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb05ddf-485f-483a-a928-43aeb1954d21",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Learn Level 3: Flash Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75434ed7-7ed9-4bf0-a48b-c3d582758f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionL3Flash(nn.Module):\n",
    "    def __init__(self, block_size, n_head, n_embed, dropout, bias):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.residual_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        attn = F.scaled_dot_product_attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.dropout if self.training else 0,\n",
    "            is_causal=True,\n",
    "            scale=1.0 / math.sqrt(k.size(-1)),\n",
    "        )\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.residual_dropout(self.c_proj(y))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e3d0ae5-0a3a-4ec7-bdb9-5def3f4cf039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yl3f = MultiHeadAttentionL3Flash(\n",
    "    block_size=block_size, n_embed=n_embed, n_head=n_head, bias=bias, dropout=dropout\n",
    ")(x)\n",
    "yl3f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b2cb3-b7c2-4e90-8a59-853475231173",
   "metadata": {},
   "source": [
    "## Learn Lever 4: Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44b0a8a7-d401-4957-84e0-b556bf26d0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 48])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttentionL4(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block_size: int,\n",
    "        n_embed: int,\n",
    "        n_heads: int,\n",
    "        dropout: float,\n",
    "        bias: bool\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_heads == 0\n",
    "\n",
    "        self.n_embed = n_embed\n",
    "        self.hs = n_embed // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.c_attn = nn.Linear(n_embed, 3 * n_embed, bias=bias)\n",
    "        self.c_proj = nn.Linear(n_embed, n_embed, bias=bias)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.dropout_proj = nn.Dropout(dropout)\n",
    "\n",
    "        ltm = torch.tril(torch.ones(block_size, block_size)).view(\n",
    "            1, 1, block_size, block_size\n",
    "        )\n",
    "        self.register_buffer(\"causal_mask\", ltm)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.size()\n",
    "        assert C == self.n_embed\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embed, dim=-1)\n",
    "\n",
    "        q = q.view(B, T, self.n_heads, self.hs).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_heads, self.hs).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_heads, self.hs).transpose(1, 2)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * (1 / math.sqrt(k.size(-1)))\n",
    "        attn = attn.masked_fill(self.causal_mask == 0, float(\"-inf\"))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout_attn(attn)\n",
    "\n",
    "        y = attn @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout_proj(self.c_proj(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "mha_l4 =  MultiHeadAttentionL4(\n",
    "    block_size=block_size, n_embed=n_embed, n_heads=n_head, bias=bias, dropout=dropout\n",
    ")(x)\n",
    "mha_l4.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
